# -*- coding: utf-8 -*-
"""Civilisation Rl Wrappers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x2nwp7rU12XpRfpQHz6ARST8axYZ1saK
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque

# ----------------------
# 1. DQN Agent Wrapper
# ----------------------

# Basic deep Q-network model using feedforward layers
class DQN(nn.Module):
    def __init__(self, obs_dim, act_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, 64), nn.ReLU(),
            nn.Linear(64, act_dim)
        )

    def forward(self, x):
        return self.net(x)

# DQN agent with ε-greedy exploration, replay buffer and target network
class DQNAgent:
    def __init__(self, obs_dim, act_dim, gamma=0.99, lr=1e-3):
        self.q_net = DQN(obs_dim, act_dim)  # Main Q-network
        self.target_net = DQN(obs_dim, act_dim)  # Target Q-network for stable updates
        self.target_net.load_state_dict(self.q_net.state_dict())
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        self.gamma = gamma
        self.epsilon = 1.0  # Exploration rate for ε-greedy
        self.memory = deque(maxlen=10000)  # Experience replay buffer
        self.obs_dim = obs_dim
        self.act_dim = act_dim

    def select_action(self, obs):
        # Choose action using ε-greedy exploration
        if random.random() < self.epsilon:
            return random.randint(0, self.act_dim - 1)
        with torch.no_grad():
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
            return self.q_net(obs_tensor).argmax().item()

    def store(self, s, a, r, s_, done):
        # Store experience tuple (s, a, r, s', done)
        self.memory.append((s, a, r, s_, done))

    def train(self, batch_size=64):
        # Train on a random batch from memory
        if len(self.memory) < batch_size:
            return

        batch = random.sample(self.memory, batch_size)
        s, a, r, s_, done = zip(*batch)
        s = torch.FloatTensor(s)
        a = torch.LongTensor(a).unsqueeze(1)
        r = torch.FloatTensor(r).unsqueeze(1)
        s_ = torch.FloatTensor(s_)
        done = torch.FloatTensor(done).unsqueeze(1)

        # Compute target Q values
        q_val = self.q_net(s).gather(1, a)
        with torch.no_grad():
            q_next = self.target_net(s_).max(1, keepdim=True)[0]
            q_target = r + self.gamma * q_next * (1 - done)

        # Update Q-network by minimizing loss
        loss = nn.MSELoss()(q_val, q_target)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def update_target(self):
        # Update target network weights from Q-network
        self.target_net.load_state_dict(self.q_net.state_dict())


# ----------------------
# 2. QMIX Agent Wrapper
# ----------------------

# Local agent Q-network used by each agent in QMIX
class AgentNet(nn.Module):
    def __init__(self, obs_dim, act_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, act_dim)
        )

    def forward(self, x):
        return self.net(x)

# Mixing network used to combine individual Q-values into global Q_total
class MixingNet(nn.Module):
    def __init__(self, n_agents, state_dim):
        super().__init__()
        self.n_agents = n_agents
        # Hypernetworks generate weight matrices based on state
        self.hyper_w1 = nn.Linear(state_dim, n_agents * 32)
        self.hyper_b1 = nn.Linear(state_dim, 32)
        self.hyper_w2 = nn.Linear(state_dim, 32)
        self.hyper_b2 = nn.Linear(state_dim, 1)

    def forward(self, agent_qs, state):
        # Combine agent Q-values into total Q-value
        bs = agent_qs.size(0)
        w1 = torch.abs(self.hyper_w1(state)).view(bs, self.n_agents, 32)
        b1 = self.hyper_b1(state).view(bs, 1, 32)
        hidden = torch.bmm(agent_qs.view(bs, 1, self.n_agents), w1) + b1
        hidden = torch.relu(hidden)
        w2 = torch.abs(self.hyper_w2(state)).view(bs, 32, 1)
        b2 = self.hyper_b2(state).view(bs, 1, 1)
        q_tot = torch.bmm(hidden, w2) + b2
        return q_tot.squeeze(-1)

# Manager class to hold multiple agents and mixing network
class QMIXSystem:
    def __init__(self, n_agents, obs_dim, act_dim, state_dim):
        self.n_agents = n_agents
        self.agent_nets = [AgentNet(obs_dim, act_dim) for _ in range(n_agents)]
        self.mix_net = MixingNet(n_agents, state_dim)
        self.target_agents = [AgentNet(obs_dim, act_dim) for _ in range(n_agents)]
        for target, agent in zip(self.target_agents, self.agent_nets):
            target.load_state_dict(agent.state_dict())
        self.optimizer = optim.Adam([p for net in self.agent_nets for p in net.parameters()] + list(self.mix_net.parameters()), lr=1e-3)
        self.memory = []  # Centralized buffer
        self.gamma = 0.99

    def store(self, transition):
        self.memory.append(transition)  # Store joint transition

    def train(self, batch_size):
        # Placeholder: implement batch training with full QMIX logic
        pass


# ----------------------
# 3. MAPPO Agent Wrapper
# ----------------------

# Combined actor-critic module for MAPPO
class MAPPOActorCritic(nn.Module):
    def __init__(self, obs_dim, act_dim):
        super().__init__()
        self.actor = nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, act_dim)
        )
        self.critic = nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, 1)
        )

    def act(self, obs):
        # Compute policy logits and sample action
        logits = self.actor(obs)
        probs = torch.distributions.Categorical(logits=logits)
        action = probs.sample()
        return action, probs.log_prob(action), probs.entropy()

    def evaluate(self, obs):
        # Compute value estimate for state
        return self.critic(obs)

# Clipped surrogate loss for PPO

def ppo_loss(old_log_probs, new_log_probs, adv, clip_ratio=0.2):
    ratio = torch.exp(new_log_probs - old_log_probs)
    clipped = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio) * adv
    return -torch.min(ratio * adv, clipped).mean()


# ----------------------
# Notes for integration:
# ----------------------
# - State representation: encode per-cell or per-tribe as fixed-length vector
# - Action space: [gather, grow, expand] → encoded as 0, 1, 2
# - Rewards: based on population growth, expansion, food collected, etc.
# - Use .select_action() / .act() in take_turn() to replace random behavior
# - Use .store(...) and .train() to accumulate experience and update policy
# - QMIX and MAPPO will require extra buffer management and trajectory storage